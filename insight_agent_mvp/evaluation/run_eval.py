#!/usr/bin/env python3
"""
LangGraphè¯„æµ‹è„šæœ¬
è¯„ä¼°è‡ªç„¶è¯­è¨€ç†è§£å‡†ç¡®ç‡ã€ç«¯åˆ°ç«¯å‡†ç¡®ç‡ã€å“åº”æ—¶é—´ç­‰æŒ‡æ ‡
"""

import json
import asyncio
import time
import sys
import os
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.insert(0, project_root)

from src.langgraph_orchestrator import process_query_with_langgraph
from src.config import OPENAI_API_KEY

@dataclass
class EvalResult:
    """è¯„æµ‹ç»“æœæ•°æ®ç±»"""
    test_id: str
    query: str
    expected_intent: Dict[str, Any]
    actual_result: Dict[str, Any]
    nlu_correct: bool
    end_to_end_correct: bool
    response_time: float
    error_message: str = ""
    category: str = ""
    description: str = ""

class LangGraphEvaluator:
    """LangGraphè¯„æµ‹å™¨"""
    
    def __init__(self, dataset_path: str):
        self.dataset_path = dataset_path
        self.results: List[EvalResult] = []
        self.start_time = None
        self.end_time = None
        
    def load_dataset(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
        try:
            with open(self.dataset_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            return []
    
    def check_nlu_accuracy(self, expected_intent: Dict, actual_result: Dict) -> bool:
        """æ£€æŸ¥è‡ªç„¶è¯­è¨€ç†è§£å‡†ç¡®ç‡"""
        if not actual_result.get("success", False):
            # å¦‚æœå®é™…ç»“æœå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯é¢„æœŸçš„é”™è¯¯
            if "error" in expected_intent:
                return True
            return False
        
        if "error" in expected_intent:
            # é¢„æœŸé”™è¯¯ä½†å®é™…æˆåŠŸ
            return False
        
        actual_intent = actual_result.get("parsed_intent", {})
        
        # æ£€æŸ¥å…³é”®å­—æ®µ
        key_fields = ["ticker", "metric", "year", "form_type"]
        for field in key_fields:
            if field in expected_intent:
                actual_val = actual_intent.get(field)
                expected_val = expected_intent[field]
                
                # ç‰¹æ®Šå¤„ç†å¹´ä»½ï¼šå…è®¸å­—ç¬¦ä¸²å’Œæ•´æ•°äº’ç›¸åŒ¹é…
                if field == "year":
                    try:
                        if int(actual_val) != int(expected_val):
                            return False
                    except (ValueError, TypeError):
                        return False
                else:
                    if actual_val != expected_val:
                        return False
        
        return True
    
    def check_end_to_end_accuracy(self, expected_intent: Dict, actual_result: Dict) -> bool:
        """æ£€æŸ¥ç«¯åˆ°ç«¯å‡†ç¡®ç‡"""
        if not actual_result.get("success", False):
            # å¦‚æœå®é™…ç»“æœå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯é¢„æœŸçš„é”™è¯¯
            if "error" in expected_intent:
                return True
            return False
        
        if "error" in expected_intent:
            # é¢„æœŸé”™è¯¯ä½†å®é™…æˆåŠŸ
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æå–çš„ç»“æœ
        result = actual_result.get("result")
        if not result:
            return False
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
        required_fields = ["ticker", "metric", "year", "value", "unit"]
        for field in required_fields:
            if field not in result:
                return False
        
        # æ£€æŸ¥tickerå’Œmetricæ˜¯å¦åŒ¹é…
        if result.get("ticker") != expected_intent.get("ticker"):
            return False
        if result.get("metric") != expected_intent.get("metric"):
            return False
        
        return True
    
    async def evaluate_single_query(self, test_case: Dict) -> EvalResult:
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        test_id = test_case.get("id", "unknown")
        query = test_case.get("query", "")
        expected_intent = test_case.get("expected_intent", {})
        category = test_case.get("category", "")
        description = test_case.get("description", "")
        
        print(f"ğŸ“‹ æµ‹è¯• {test_id}: {query}")
        
        # æµ‹é‡å“åº”æ—¶é—´
        start_time = time.time()
        
        try:
            # ä½¿ç”¨LangGraphå¤„ç†æŸ¥è¯¢
            actual_result = await process_query_with_langgraph(query)
            response_time = time.time() - start_time
            
            # æ£€æŸ¥NLUå‡†ç¡®ç‡
            nlu_correct = self.check_nlu_accuracy(expected_intent, actual_result)
            
            # æ£€æŸ¥ç«¯åˆ°ç«¯å‡†ç¡®ç‡
            end_to_end_correct = self.check_end_to_end_accuracy(expected_intent, actual_result)
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            result = EvalResult(
                test_id=test_id,
                query=query,
                expected_intent=expected_intent,
                actual_result=actual_result,
                nlu_correct=nlu_correct,
                end_to_end_correct=end_to_end_correct,
                response_time=response_time,
                category=category,
                description=description
            )
            
            # è¾“å‡ºç»“æœ
            status = "âœ…" if nlu_correct and end_to_end_correct else "âŒ"
            print(f"  {status} NLU: {nlu_correct}, E2E: {end_to_end_correct}, è€—æ—¶: {response_time:.2f}s")
            
            return result
            
        except Exception as e:
            response_time = time.time() - start_time
            error_message = str(e)
            
            result = EvalResult(
                test_id=test_id,
                query=query,
                expected_intent=expected_intent,
                actual_result={"error": error_message, "success": False},
                nlu_correct=False,
                end_to_end_correct=False,
                response_time=response_time,
                error_message=error_message,
                category=category,
                description=description
            )
            
            print(f"  âŒ æ‰§è¡Œå¤±è´¥: {error_message}")
            return result
    
    async def run_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´è¯„æµ‹"""
        print("ğŸ§ª å¼€å§‹LangGraphè¯„æµ‹")
        print("=" * 60)
        
        # æ£€æŸ¥APIå¯†é’¥
        if not OPENAI_API_KEY:
            print("âŒ ç¼ºå°‘OPENAI_API_KEYç¯å¢ƒå˜é‡")
            return {"error": "ç¼ºå°‘OPENAI_API_KEYç¯å¢ƒå˜é‡"}
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset()
        if not dataset:
            return {"error": "æ•°æ®é›†åŠ è½½å¤±è´¥"}
        
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}ä¸ªæµ‹è¯•ç”¨ä¾‹")
        print("=" * 60)
        
        self.start_time = datetime.now()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        for test_case in dataset:
            result = await self.evaluate_single_query(test_case)
            self.results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
            await asyncio.sleep(0.5)
        
        self.end_time = datetime.now()
        
        # ç”ŸæˆæŠ¥å‘Š
        return self.generate_report()
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        if not self.results:
            return {"error": "æ²¡æœ‰è¯„æµ‹ç»“æœ"}
        
        total_tests = len(self.results)
        nlu_correct = sum(1 for r in self.results if r.nlu_correct)
        e2e_correct = sum(1 for r in self.results if r.end_to_end_correct)
        
        nlu_accuracy = nlu_correct / total_tests * 100
        e2e_accuracy = e2e_correct / total_tests * 100
        
        avg_response_time = sum(r.response_time for r in self.results) / total_tests
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = {}
        for result in self.results:
            category = result.category
            if category not in category_stats:
                category_stats[category] = {
                    "total": 0,
                    "nlu_correct": 0,
                    "e2e_correct": 0,
                    "avg_response_time": 0
                }
            
            category_stats[category]["total"] += 1
            if result.nlu_correct:
                category_stats[category]["nlu_correct"] += 1
            if result.end_to_end_correct:
                category_stats[category]["e2e_correct"] += 1
            category_stats[category]["avg_response_time"] += result.response_time
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        for category, stats in category_stats.items():
            if stats["total"] > 0:
                stats["nlu_accuracy"] = stats["nlu_correct"] / stats["total"] * 100
                stats["e2e_accuracy"] = stats["e2e_correct"] / stats["total"] * 100
                stats["avg_response_time"] = stats["avg_response_time"] / stats["total"]
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = {
            "summary": {
                "total_tests": total_tests,
                "nlu_accuracy": nlu_accuracy,
                "e2e_accuracy": e2e_accuracy,
                "avg_response_time": avg_response_time,
                "evaluation_time": str(self.end_time - self.start_time)
            },
            "category_stats": category_stats,
            "detailed_results": [
                {
                    "test_id": r.test_id,
                    "query": r.query,
                    "category": r.category,
                    "description": r.description,
                    "nlu_correct": r.nlu_correct,
                    "end_to_end_correct": r.end_to_end_correct,
                    "response_time": r.response_time,
                    "expected_intent": r.expected_intent,
                    "actual_result": r.actual_result,
                    "error_message": r.error_message
                }
                for r in self.results
            ]
        }
        
        return report
    
    def print_report(self, report: Dict[str, Any]) -> None:
        """æ‰“å°è¯„æµ‹æŠ¥å‘Š"""
        if "error" in report:
            print(f"âŒ è¯„æµ‹å¤±è´¥: {report['error']}")
            return
        
        summary = report["summary"]
        category_stats = report["category_stats"]
        
        print("=" * 60)
        print("ğŸ“Š LangGraphè¯„æµ‹æŠ¥å‘Š")
        print("=" * 60)
        
        print(f"ğŸ“‹ æ€»ä½“ç»Ÿè®¡:")
        print(f"  â€¢ æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"  â€¢ NLUå‡†ç¡®ç‡: {summary['nlu_accuracy']:.1f}%")
        print(f"  â€¢ ç«¯åˆ°ç«¯å‡†ç¡®ç‡: {summary['e2e_accuracy']:.1f}%")
        print(f"  â€¢ å¹³å‡å“åº”æ—¶é—´: {summary['avg_response_time']:.2f}s")
        print(f"  â€¢ è¯„æµ‹è€—æ—¶: {summary['evaluation_time']}")
        
        print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
        for category, stats in category_stats.items():
            print(f"  â€¢ {category}:")
            print(f"    - æµ‹è¯•æ•°: {stats['total']}")
            print(f"    - NLUå‡†ç¡®ç‡: {stats['nlu_accuracy']:.1f}%")
            print(f"    - ç«¯åˆ°ç«¯å‡†ç¡®ç‡: {stats['e2e_accuracy']:.1f}%")
            print(f"    - å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}s")
        
        print(f"\nâŒ å¤±è´¥ç”¨ä¾‹:")
        failed_cases = [r for r in report["detailed_results"] if not (r["nlu_correct"] and r["end_to_end_correct"])]
        if failed_cases:
            for case in failed_cases:
                print(f"  â€¢ {case['test_id']}: {case['query']}")
                print(f"    - NLU: {case['nlu_correct']}, E2E: {case['end_to_end_correct']}")
                if case['error_message']:
                    print(f"    - é”™è¯¯: {case['error_message']}")
        else:
            print("  ğŸ‰ æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹éƒ½é€šè¿‡äº†ï¼")
        
        print("=" * 60)
    
    def save_report(self, report: Dict[str, Any], output_path: str) -> None:
        """ä¿å­˜è¯„æµ‹æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„
    current_dir = os.path.dirname(__file__)
    dataset_path = os.path.join(current_dir, "eval_dataset.json")
    
    # åˆ›å»ºreportså­æ–‡ä»¶å¤¹
    reports_dir = os.path.join(current_dir, "reports")
    os.makedirs(reports_dir, exist_ok=True)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æŠ¥å‘Šæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(reports_dir, f"eval_report_{timestamp}.json")
    
    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = LangGraphEvaluator(dataset_path)
    
    # è¿è¡Œè¯„æµ‹
    report = await evaluator.run_evaluation()
    
    # æ‰“å°æŠ¥å‘Š
    evaluator.print_report(report)
    
    # ä¿å­˜æŠ¥å‘Š
    if "error" not in report:
        evaluator.save_report(report, report_path)

if __name__ == "__main__":
    asyncio.run(main()) 